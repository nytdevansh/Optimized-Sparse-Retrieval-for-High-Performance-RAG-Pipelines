# Default benchmark configuration

# Core settings
seed: 42
device: "cpu"
num_threads: -1  # -1 means use all available

# Dataset settings
dataset:
  name: "fiqa"  # one of: fiqa, nq, msmarco, micro
  subset_size: null  # null means use full dataset
  validation: true  # validate dataset before running

# Benchmark settings
benchmark:
  warmup_queries: 10
  sample_size: 100
  measure_index_build: true
  measure_memory: true
  save_raw_results: true

# Metrics settings
metrics:
  - "nDCG@10"
  - "MRR@10"
  - "Recall@100"
  - "MAP@100"

# Performance targets
targets:
  bm25:
    speedup: 8.0  # vs baseline
    memory_reduction: 4.0  # vs baseline
    max_quality_drop: 0.015  # nDCG drop tolerance
    
  dense:
    speedup: 3.0  # vs float32
    memory_reduction: 4.0  # vs float32
    max_quality_drop: 0.015  # nDCG drop tolerance
    
  topk:
    speedup: 5.0  # vs heapq
    batch_size: 1000  # for batch processing

# Method-specific settings
methods:
  bm25:
    k1: 1.2
    b: 0.75
    use_simd: true
    use_numba: true
    
  dense:
    model_name: "sentence-transformers/all-MiniLM-L6-v2"
    quantization: "int8"  # one of: none, int8, int4
    batch_size: 32
    max_length: 128
    
  hybrid:
    weights:
      bm25: 0.5
      dense: 0.5
    aggregation: "interpolation"  # one of: interpolation, reciprocal_rank

# Output settings
output:
  results_dir: "results"
  save_checkpoints: true
  report_format: "markdown"
  wandb:
    enabled: false
    project: "lightning-retrieval"