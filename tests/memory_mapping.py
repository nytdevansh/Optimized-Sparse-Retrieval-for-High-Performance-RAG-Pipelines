"""
Memory Mapping Performance Tests
Tests for memory-mapped file I/O optimizations
"""

import os
import sys
import mmap
import json
import time
import struct
import tempfile
import numpy as np
from pathlib import Path
from typing import List, Dict, Iterator, Tuple, Optional
import psutil
from contextlib import contextmanager


import zlib
import gc

class BinaryCorpusBuilder:
    """Build efficient binary corpus format with optional compression"""
    
    def __init__(self, compression_level: int = 1):
        self.doc_format = struct.Struct('<Q Q Q B')  # doc_id_len, text_len, title_len, flags
        self.offset_format = struct.Struct('<Q')  # 8-byte offset
        self.compression_level = compression_level
        self.alignment = 16  # Reduced from 64 to save space
        self.BUFFER_SIZE = 8 * 1024 * 1024  # 8MB buffer for better performance
        
    def encode_text(self, text: str) -> bytes:
        """Safely encode text as UTF-8"""
        return str(text).encode('utf-8', errors='replace')
        
    def compress_text(self, text: bytes) -> tuple[bytes, int]:
        """Compress text if beneficial"""
        if len(text) < 100:  # Don't compress small texts
            return text, 0
        try:
            compressed = zlib.compress(text, level=self.compression_level)
            # Only use compression if it actually saves space
            if len(compressed) < len(text):
                return compressed, 1
        except Exception:
            pass  # Fall back to uncompressed on any error
        return text, 0
    
    def build_binary_corpus(self, documents: List[Dict], output_path: str) -> Dict[str, int]:
        """Build binary corpus with offset index and optional compression"""
        corpus_path = Path(output_path)
        index_path = corpus_path.with_suffix('.idx')
        
        stats = {
            'total_docs': 0,
            'total_bytes': 0,
            'avg_doc_length': 0,
            'compressed_docs': 0,
            'original_size': 0,
            'compressed_size': 0
        }
        doc_lengths = []
        
        with open(corpus_path, 'wb') as corpus_file, \
             open(index_path, 'wb') as index_file:
            
            for doc in documents:
                offset = corpus_file.tell()
                
                # Write offset to index
                index_file.write(self.offset_format.pack(offset))
                
                # Prepare and potentially compress document data
                doc_id = self.encode_text(doc['_id'])
                text_bytes = self.encode_text(doc.get('text', ''))
                title_bytes = self.encode_text(doc.get('title', ''))
                
                # Try compressing text if it's large enough
                text_data, is_compressed = self.compress_text(text_bytes)
                title_data, title_compressed = self.compress_text(title_bytes)
                
                # Update compression stats
                if is_compressed or title_compressed:
                    stats['compressed_docs'] += 1
                    stats['original_size'] += len(text_bytes) + len(title_bytes)
                    stats['compressed_size'] += len(text_data) + len(title_data)
                
                # Create flags (bit 0: text compressed, bit 1: title compressed)
                flags = (is_compressed | (title_compressed << 1))
                
                # Write document header
                corpus_file.write(self.doc_format.pack(
                    len(doc_id),
                    len(text_data),
                    len(title_data),
                    flags
                ))
                
                # Write document data
                corpus_file.write(doc_id)
                corpus_file.write(text_data)
                corpus_file.write(title_data)
                
                # Use smaller alignment and only pad if needed for performance
                current_pos = corpus_file.tell()
                if current_pos % self.alignment != 0:
                    padding = (self.alignment - (current_pos % self.alignment))
                    corpus_file.write(b'\x00' * padding)
                
                doc_lengths.append(len(text_bytes))  # Use original length for stats
                stats['total_docs'] += 1
                stats['total_bytes'] = corpus_file.tell()
        
        stats['avg_doc_length'] = sum(doc_lengths) / len(doc_lengths) if doc_lengths else 0
        
        # Write metadata
        meta_path = corpus_path.with_suffix('.meta')
        with open(meta_path, 'w') as f:
            json.dump(stats, f, indent=2)
        
        return stats


from collections import OrderedDict
import threading

class LRUCache:
    """Thread-safe LRU cache implementation"""
    
    def __init__(self, capacity: int):
        self.cache = OrderedDict()
        self.capacity = capacity
        self.lock = threading.Lock()
    
    def get(self, key) -> Optional[Dict[str, str]]:
        """Get item from cache, return None if not found"""
        with self.lock:
            if key not in self.cache:
                return None
            # Move to end (most recently used)
            value = self.cache.pop(key)
            self.cache[key] = value
            return value
    
    def put(self, key, value):
        """Put item in cache, evicting least recently used if needed"""
        with self.lock:
            if key in self.cache:
                self.cache.pop(key)
            elif len(self.cache) >= self.capacity:
                # Remove least recently used
                self.cache.popitem(last=False)
            self.cache[key] = value

class MemoryMappedCorpus:
    """Memory-mapped corpus reader with LRU cache and buffering"""
    
    def __init__(self, corpus_path: str, cache_size: int = 10000,
                 buffer_size: int = 1024 * 1024):  # 1MB buffer
        self.corpus_path = Path(corpus_path)
        self.index_path = self.corpus_path.with_suffix('.idx')
        
        # Memory map files
        self.corpus_file = open(self.corpus_path, 'rb')
        self.corpus_mmap = mmap.mmap(self.corpus_file.fileno(), 0, access=mmap.ACCESS_READ)
        
        self.index_file = open(self.index_path, 'rb')
        self.index_mmap = mmap.mmap(self.index_file.fileno(), 0, access=mmap.ACCESS_READ)
        
        # Match BinaryCorpusBuilder format
        self.doc_format = struct.Struct('<Q Q Q B')  # doc_id_len, text_len, title_len, flags
        self.offset_format = struct.Struct('<Q')  # 8-byte offset
        
        # Initialize LRU cache
        self.cache = LRUCache(cache_size)
        
        # Calculate number of documents
        self.num_docs = os.path.getsize(self.index_path) // self.offset_format.size
        
        # Read-ahead buffer settings
        self.buffer_size = buffer_size
        self.buffer = None
        self.buffer_start = 0
        self.buffer_end = 0
    
    def __len__(self) -> int:
        return self.num_docs
    
    def _ensure_buffer(self, offset: int, size: int):
        """Ensure the requested range is in the buffer"""
        if (self.buffer is None or
            offset < self.buffer_start or
            offset + size > self.buffer_end):
            
            # Calculate new buffer boundaries
            self.buffer_start = offset
            self.buffer_end = min(offset + self.buffer_size,
                                len(self.corpus_mmap))
            
            # Read new buffer
            self.buffer = memoryview(
                self.corpus_mmap[self.buffer_start:self.buffer_end]
            )
    
    def _read_document_at_offset(self, offset: int) -> Dict[str, str]:
        """Read document data at given offset"""
        # Read header (25 bytes: two uint64s, one uint64, and one uint8)
        self._ensure_buffer(offset, 25)
        header_pos = offset - self.buffer_start
        doc_id_len, text_len, title_len, flags = self.doc_format.unpack(
            self.buffer[header_pos:header_pos + 25]
        )
        
        # Read full document data
        total_size = 25 + doc_id_len + text_len + title_len
        self._ensure_buffer(offset, total_size)
        
        # Extract document data with error handling
        data_pos = header_pos + 25
        try:
            # Read document ID
            doc_id = self.buffer[data_pos:data_pos + doc_id_len].tobytes().decode('utf-8', errors='replace')
            
            # Read text (potentially compressed)
            text_pos = data_pos + doc_id_len
            text_data = self.buffer[text_pos:text_pos + text_len].tobytes()
            try:
                if flags & 1:  # Text is compressed
                    text = zlib.decompress(text_data).decode('utf-8', errors='replace')
                else:
                    text = text_data.decode('utf-8', errors='replace')
            except zlib.error:
                # Fallback if decompression fails
                text = text_data.decode('utf-8', errors='replace')
            
            # Read title (potentially compressed)
            title_pos = text_pos + text_len
            title_data = self.buffer[title_pos:title_pos + title_len].tobytes()
            try:
                if flags & 2:  # Title is compressed
                    title = zlib.decompress(title_data).decode('utf-8', errors='replace')
                else:
                    title = title_data.decode('utf-8', errors='replace')
            except zlib.error:
                # Fallback if decompression fails
                title = title_data.decode('utf-8', errors='replace')
            
        except Exception as e:
            # Log error details for debugging
            raw_data = self.buffer[data_pos:data_pos + doc_id_len + text_len + title_len].tobytes()
            raise RuntimeError(f"Failed to decode document data: {e}\nRaw data: {raw_data!r}")
        
        return {
            '_id': doc_id,
            'text': text,
            'title': title
        }
    
    def __getitem__(self, doc_idx: int) -> Dict[str, str]:
        """Get document by index using cache and memory mapping"""
        if doc_idx >= len(self):
            raise IndexError(f"Document index {doc_idx} out of range")
        
        # Check cache first
        cached_doc = self.cache.get(doc_idx)
        if cached_doc is not None:
            return cached_doc
        
        # Get offset from index
        offset_pos = doc_idx * self.offset_format.size
        offset_bytes = self.index_mmap[offset_pos:offset_pos + self.offset_format.size]
        offset = self.offset_format.unpack(offset_bytes)[0]
        
        # Read document and update cache
        doc = self._read_document_at_offset(offset)
        self.cache.put(doc_idx, doc)
        
        return doc
    
    def get_batch(self, start_idx: int, batch_size: int) -> List[Dict[str, str]]:
        """Get batch of documents with optimized sequential access"""
        end_idx = min(start_idx + batch_size, len(self))
        batch = []
        
        # Process documents sequentially for better buffer utilization
        for idx in range(start_idx, end_idx):
            # Try cache first
            cached_doc = self.cache.get(idx)
            if cached_doc is not None:
                batch.append(cached_doc)
                continue
            
            # Get offset from index
            offset_pos = idx * self.offset_format.size
            offset_bytes = self.index_mmap[offset_pos:offset_pos + self.offset_format.size]
            offset = self.offset_format.unpack(offset_bytes)[0]
            
            # Read document and update cache
            doc = self._read_document_at_offset(offset)
            self.cache.put(idx, doc)
            batch.append(doc)
        
        return batch
    
    def close(self):
        """Clean up resources"""
        if hasattr(self, 'buffer'):
            self.buffer = None
        if hasattr(self, 'cache'):
            self.cache.cache.clear()
        if hasattr(self, 'corpus_mmap'):
            self.corpus_mmap.close()
        if hasattr(self, 'index_mmap'):
            self.index_mmap.close()
        if hasattr(self, 'corpus_file'):
            self.corpus_file.close()
        if hasattr(self, 'index_file'):
            self.index_file.close()
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()
    
    def __del__(self):
        """Cleanup on garbage collection"""
        self.close()


class StandardCorpusReader:
    """Standard file-based corpus reader for comparison"""
    
    def __init__(self, corpus_path: str):
        self.corpus_path = Path(corpus_path)
        
        # Load entire corpus into memory
        self.documents = []
        with open(self.corpus_path, 'r') as f:
            for line in f:
                doc = json.loads(line.strip())
                self.documents.append(doc)
    
    def __len__(self) -> int:
        return len(self.documents)
    
    def __getitem__(self, doc_idx: int) -> Dict[str, str]:
        return self.documents[doc_idx]
    
    def get_batch(self, start_idx: int, batch_size: int) -> List[Dict[str, str]]:
        end_idx = min(start_idx + batch_size, len(self))
        return self.documents[start_idx:end_idx]


class LazyJSONLReader:
    """Lazy JSONL reader that doesn't load everything into memory"""
    
    def __init__(self, corpus_path: str):
        self.corpus_path = Path(corpus_path)
        
        # Build line index for random access
        self.line_offsets = []
        with open(self.corpus_path, 'rb') as f:
            offset = 0
            while True:
                self.line_offsets.append(offset)
                line = f.readline()
                if not line:
                    break
                offset = f.tell()
        
        # Remove last empty offset
        if self.line_offsets and offset == self.line_offsets[-1]:
            self.line_offsets.pop()
    
    def __len__(self) -> int:
        return len(self.line_offsets)
    
    def __getitem__(self, doc_idx: int) -> Dict[str, str]:
        if doc_idx >= len(self):
            raise IndexError(f"Document index {doc_idx} out of range")
        
        with open(self.corpus_path, 'r') as f:
            f.seek(self.line_offsets[doc_idx])
            line = f.readline().strip()
            return json.loads(line)
    
    def get_batch(self, start_idx: int, batch_size: int) -> List[Dict[str, str]]:
        end_idx = min(start_idx + batch_size, len(self))
        
        documents = []
        with open(self.corpus_path, 'r') as f:
            for i in range(start_idx, end_idx):
                f.seek(self.line_offsets[i])
                line = f.readline().strip()
                documents.append(json.loads(line))
        
        return documents


class MemoryMappingTestSuite:
    """Test suite for memory mapping optimizations"""
    
    def __init__(self):
        self.temp_dir = None
        self.corpus_builder = BinaryCorpusBuilder()
        self.corpus_path = None  # Path to original JSONL file
        self.binary_path = None  # Path to binary converted file
    
    @contextmanager
    def temp_directory(self):
        """Context manager for temporary directory"""
        self.temp_dir = tempfile.mkdtemp(prefix='mmap_test_')
        try:
            yield Path(self.temp_dir)
        finally:
            # Cleanup
            import shutil
            shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def generate_test_corpus(self, num_docs: int = 50000, 
                           avg_doc_length: int = 200) -> List[Dict[str, str]]:
        """Generate test corpus with realistic document sizes"""
        np.random.seed(42)
        
        documents = []
        vocab = [f"word_{i}" for i in range(10000)]
        
        for doc_id in range(num_docs):
            # Variable document length (gamma distribution)
            doc_length = max(10, int(np.random.gamma(2, avg_doc_length / 2)))
            
            # Generate document text
            words = np.random.choice(vocab, size=doc_length)
            text = ' '.join(words)
            
            documents.append({
                '_id': f"doc_{doc_id:06d}",
                'title': f"Document {doc_id}",
                'text': text
            })
        
        return documents
    
    def write_jsonl_corpus(self, documents: List[Dict], output_path: str):
        """Write documents to JSONL format"""
        with open(output_path, 'w') as f:
            for doc in documents:
                f.write(json.dumps(doc) + '\n')
    
    def test_corpus_creation_speed(self, documents: List[Dict]) -> Dict[str, float]:
        """Test speed of different corpus creation methods"""
        print("Testing corpus creation speed...")
        
        with self.temp_directory() as temp_dir:
            jsonl_path = temp_dir / "corpus.jsonl"
            binary_path = temp_dir / "corpus.bin"
            
            # Save paths for other tests
            self.corpus_path = jsonl_path
            self.binary_path = binary_path
            
            # Test JSONL creation
            start = time.perf_counter()
            self.write_jsonl_corpus(documents, jsonl_path)
            jsonl_time = time.perf_counter() - start
            
            # Test binary corpus creation
            start = time.perf_counter()
            stats = self.corpus_builder.build_binary_corpus(documents, binary_path)
            binary_time = time.perf_counter() - start
            
            # Get file sizes
            jsonl_size = os.path.getsize(jsonl_path)
            binary_size = os.path.getsize(binary_path)
            index_size = os.path.getsize(binary_path.with_suffix('.idx'))
            
            results = {
                'jsonl_creation_time': jsonl_time,
                'binary_creation_time': binary_time,
                'jsonl_size_mb': jsonl_size / (1024 * 1024),
                'binary_size_mb': binary_size / (1024 * 1024),
                'index_size_mb': index_size / (1024 * 1024),
                'total_binary_size_mb': (binary_size + index_size) / (1024 * 1024),
                'compression_ratio': jsonl_size / (binary_size + index_size)
            }
            
            print(f"JSONL creation: {jsonl_time:.3f}s, size: {results['jsonl_size_mb']:.1f}MB")
            print(f"Binary creation: {binary_time:.3f}s, size: {results['total_binary_size_mb']:.1f}MB")
            print(f"Compression ratio: {results['compression_ratio']:.2f}x")
            
            return results
    
    def test_random_access_performance(self, documents: List[Dict]) -> Dict[str, float]:
        """Test random access performance across different implementations"""
        print("Testing random access performance...")
        
        with self.temp_directory() as temp_dir:
            jsonl_path = temp_dir / "corpus.jsonl"
            binary_path = temp_dir / "corpus.bin"
            
            # Create test files
            self.write_jsonl_corpus(documents, jsonl_path)
            self.corpus_builder.build_binary_corpus(documents, binary_path)
            
            # Initialize lazy reader and memory-mapped reader
            lazy_reader = LazyJSONLReader(jsonl_path)
            mmap_reader = MemoryMappedCorpus(
                str(binary_path),
                cache_size=50000,  # Larger cache for real workloads
                buffer_size=8 * 1024 * 1024  # 8MB buffer
            )
            
            # More realistic access pattern simulation
            np.random.seed(123)
            num_patterns = 5  # Number of distinct access patterns
            accesses_per_pattern = 200
            
            access_patterns = []
            for _ in range(num_patterns):
                # Simulate "query processing" - sequential access to nearby docs
                center = np.random.randint(0, len(documents))
                radius = np.random.randint(10, 30)
                pattern = np.random.randint(
                    max(0, center - radius),
                    min(len(documents), center + radius),
                    size=accesses_per_pattern
                )
                access_patterns.extend(pattern)
            
            # Benchmark each reader
            readers = {
                'lazy_jsonl': lazy_reader,
                'memory_mapped': mmap_reader
            }
            
            # Add proper warmup phase
            warmup_indices = np.random.choice(len(documents), size=100)
            for name, reader in readers.items():
                for idx in warmup_indices:
                    _ = reader[idx]
            
            results = {}
            num_accesses = len(access_patterns)
            
            # Actual benchmark
            for name, reader in readers.items():
                print(f"Benchmarking {name} reader...")
                
                start = time.perf_counter()
                for idx in access_patterns:
                    doc = reader[idx]
                    # Simulate document processing
                    _ = len(doc['text'])
                end = time.perf_counter()
                
                total_time = end - start
                avg_latency = (total_time / num_accesses) * 1000  # ms per access
                
                results[f'{name}_total_time'] = total_time
                results[f'{name}_avg_latency_ms'] = avg_latency
                
                print(f"  {name}: {total_time:.3f}s total, {avg_latency:.3f}ms per access")
            
            # Calculate speedup relative to lazy_jsonl
            baseline_time = results['lazy_jsonl_total_time']
            results['mmap_speedup'] = baseline_time / results['memory_mapped_total_time']
            
            return results
    
    def test_sequential_access_performance(self, documents: List[Dict]) -> Dict[str, float]:
        """Test sequential access performance"""
        print("Testing sequential access performance...")
        
        with self.temp_directory() as temp_dir:
            jsonl_path = temp_dir / "corpus.jsonl"
            binary_path = temp_dir / "corpus.bin"
            
            # Create test files
            self.write_jsonl_corpus(documents, jsonl_path)
            self.corpus_builder.build_binary_corpus(documents, binary_path)
            
            # Initialize readers
            readers = {
                'standard': StandardCorpusReader(jsonl_path),
                'lazy_jsonl': LazyJSONLReader(jsonl_path),
                'memory_mapped': MemoryMappedCorpus(binary_path)
            }
            
            results = {}
            batch_size = 100
            num_batches = min(100, len(documents) // batch_size)
            
            for name, reader in readers.items():
                print(f"Benchmarking {name} sequential access...")
                
                # Warmup
                _ = reader.get_batch(0, 10)
                
                # Actual benchmark
                start = time.perf_counter()
                total_docs = 0
                total_chars = 0
                
                for i in range(num_batches):
                    start_idx = i * batch_size
                    batch = reader.get_batch(start_idx, batch_size)
                    
                    # Simulate processing
                    for doc in batch:
                        total_docs += 1
                        total_chars += len(doc['text'])
                
                end = time.perf_counter()
                
                total_time = end - start
                docs_per_sec = total_docs / total_time
                
                results[f'{name}_sequential_time'] = total_time
                results[f'{name}_docs_per_sec'] = docs_per_sec
                
                print(f"  {name}: {total_time:.3f}s, {docs_per_sec:.0f} docs/sec")
            
            # Calculate speedups
            baseline_dps = results['standard_docs_per_sec']
            results['lazy_sequential_speedup'] = results['lazy_jsonl_docs_per_sec'] / baseline_dps
            results['mmap_sequential_speedup'] = results['memory_mapped_docs_per_sec'] / baseline_dps
            
            return results
    
    def test_memory_usage(self, documents: List[Dict]) -> Dict[str, float]:
        """Test memory usage of different readers"""
        print("Testing memory usage...")
        
        def measure_peak_memory(func):
            """Measure peak memory usage of a function"""
            process = psutil.Process()
            
            # Clear caches and collect garbage
            gc.collect()
            initial = process.memory_info().rss
            
            # Run the function and track memory
            peak = initial
            result = func()
            final = process.memory_info().rss
            peak = max(peak, final)
            
            # Return peak memory usage in MB
            memory_used = max(0, peak - initial) / (1024 * 1024)
            return memory_used, result
        
        with self.temp_directory() as temp_dir:
            jsonl_path = temp_dir / "corpus.jsonl"
            binary_path = temp_dir / "corpus.bin"
            
            # Create test files
            self.write_jsonl_corpus(documents, jsonl_path)
            self.corpus_builder.build_binary_corpus(documents, binary_path)
            
            results = {}
            
            # Measure standard reader memory usage
            def test_standard():
                reader = StandardCorpusReader(str(jsonl_path))
                # Access documents to ensure full loading
                for i in range(min(100, len(reader))):
                    _ = reader[i]
                return reader
            
            std_mem, _ = measure_peak_memory(test_standard)
            results['standard_memory_mb'] = std_mem
            
            # Clear memory before next test
            gc.collect()
            time.sleep(0.1)  # Allow OS to reclaim memory
            
            # Measure memory-mapped reader memory usage
            def test_mmap():
                reader = MemoryMappedCorpus(str(binary_path))
                # Access same number of documents
                for i in range(min(100, len(reader))):
                    _ = reader[i]
                return reader
            
            mmap_mem, _ = measure_peak_memory(test_mmap)
            results['mmap_memory_mb'] = mmap_mem
            
            # Calculate efficiency (avoid division by zero)
            mmap_memory = max(0.1, results['mmap_memory_mb'])  # Ensure non-zero denominator
            results['memory_efficiency'] = results['standard_memory_mb'] / mmap_memory
            
            print(f"Standard reader memory: {results['standard_memory_mb']:.1f}MB")
            print(f"Memory-mapped reader memory: {results['mmap_memory_mb']:.1f}MB")
            print(f"Memory efficiency: {results['memory_efficiency']:.2f}x")
            
            return results
    
    def test_cold_start_performance(self, documents: List[Dict]) -> Dict[str, float]:
        """Test cold start performance (first access after clearing caches)"""
        print("Testing cold start performance...")
        
        with self.temp_directory() as temp_dir:
            jsonl_path = temp_dir / "corpus.jsonl"
            binary_path = temp_dir / "corpus.bin"
            
            # Create test files
            self.write_jsonl_corpus(documents, jsonl_path)
            self.corpus_builder.build_binary_corpus(documents, binary_path)
            
            results = {}
            test_indices = [0, len(documents) // 4, len(documents) // 2, -1]
            
            # Test each reader type
            reader_types = [
                ('lazy_jsonl', LazyJSONLReader),
                ('memory_mapped', MemoryMappedCorpus)
            ]
            
            for name, reader_class in reader_types:
                cold_start_times = []
                
                for test_idx in test_indices:
                    # Create fresh reader instance
                    if name == 'lazy_jsonl':
                        reader = reader_class(jsonl_path)
                    else:
                        reader = reader_class(binary_path)
                    
                    # Clear OS page cache (Linux only)
                    if sys.platform.startswith('linux'):
                        os.system("sync && echo 1 > /proc/sys/vm/drop_caches")
                    # On macOS, we can't clear the page cache, so we'll just continue
                    
                        # Measure first access time
                    try:
                        start = time.perf_counter()
                        doc = reader[test_idx if test_idx >= 0 else len(reader) + test_idx]
                        end = time.perf_counter()
                        cold_start_times.append((end - start) * 1000)  # Convert to ms
                    except Exception as e:
                        print(f"Error during cold start test: {e}")
                        cold_start_times.append(0)  # Add dummy value on error
                    
                    # Cleanup
                    del reader
                
                results[f'{name}_cold_start_ms'] = np.mean(cold_start_times)
                results[f'{name}_cold_start_std'] = np.std(cold_start_times)
                
                print(f"{name} cold start: {results[f'{name}_cold_start_ms']:.2f}¬±{results[f'{name}_cold_start_std']:.2f}ms")
            
            return results


if __name__ == "__main__":
    print("=" * 60)
    print("Memory Mapping Performance Test Suite")
    print("=" * 60)
    
    suite = MemoryMappingTestSuite()
    
    # Generate test corpus
    print("Generating test corpus...")
    documents = suite.generate_test_corpus(num_docs=20000, avg_doc_length=150)
    print(f"Generated {len(documents)} documents")
    
    print("\n" + "=" * 60)
    
    # Test corpus creation speed
    creation_results = suite.test_corpus_creation_speed(documents)
    print(f"\nCreation Performance:")
    binary_faster = creation_results['jsonl_creation_time'] > creation_results['binary_creation_time']
    print(f"  Binary format creation: {'‚úÖ FASTER' if binary_faster else '‚ùå SLOWER'}")
    print(f"  Compression: {'‚úÖ GOOD' if creation_results['compression_ratio'] > 1.2 else '‚ùå POOR'} ({creation_results['compression_ratio']:.2f}x)")
    
    print("\n" + "=" * 60)
    
    # Test random access performance
    random_results = suite.test_random_access_performance(documents)
    # Test random access and sequential performance
    random_results = suite.test_random_access_performance(documents)
    print("\nRandom Access Performance:")
    print(f"  Memory-mapped speedup: {'‚úÖ GOOD' if random_results['mmap_speedup'] > 1.5 else '‚ùå POOR'} ({random_results['mmap_speedup']:.2f}x vs lazy_jsonl)")
    
    print("\n" + "=" * 60)
    
    sequential_results = suite.test_sequential_access_performance(documents)
    print(f"\nSequential Access Performance:")
    print(f"  Memory-mapped speedup: {'‚úÖ GOOD' if sequential_results['mmap_sequential_speedup'] > 1.2 else '‚ùå MARGINAL'} ({sequential_results['mmap_sequential_speedup']:.2f}x)")
    
    print("\n" + "=" * 60)
    
    # Test memory usage
    memory_results = suite.test_memory_usage(documents)
    memory_efficiency = memory_results['memory_efficiency']
    
    print(f"\nMemory Efficiency:")
    print(f"  Memory reduction: {'‚úÖ EXCELLENT' if memory_efficiency > 4.0 else '‚úÖ GOOD' if memory_efficiency > 2.0 else '‚ùå POOR'} ({memory_efficiency:.2f}x)")
    
    print("\n" + "=" * 60)
    
    # Test cold start performance
    cold_start_results = suite.test_cold_start_performance(documents)
    
    print("\n" + "=" * 60)
    print("OVERALL ASSESSMENT")
    
    # Define success criteria
    creation_good = creation_results.get('compression_ratio', 0) > 1.2
    random_good = random_results['mmap_speedup'] > 1.5  # Compare to lazy_jsonl
    memory_good = memory_efficiency > 2.0
    
    print(f"File format efficiency: {'‚úÖ PASS' if creation_good else '‚ùå FAIL'}")
    print(f"Random access speed: {'‚úÖ PASS' if random_good else '‚ùå FAIL'}")
    print(f"Memory efficiency: {'‚úÖ PASS' if memory_good else '‚ùå FAIL'}")
    
    overall_success = creation_good and random_good and memory_good
    print(f"Overall result: {'üéâ SUCCESS' if overall_success else 'üö® NEEDS WORK'}")
    
    if overall_success:
        print("\nüöÄ Memory mapping optimizations are working as expected!")
        print("   Ready for production deployment.")
    else:
        print("\n‚ö†Ô∏è  Some optimizations need improvement.")
        print("   Consider tuning file format or access patterns.")